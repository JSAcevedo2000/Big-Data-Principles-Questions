{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data principles and best practices of scalable real-time data systems.\n",
    "### Questions of chapter 1 & 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tWhat is the difference of scaling standard RDBMS and scaling Big Data designed databases?\n",
    "Standard RDBMs face many problems when it comes to \"big data\", mainly with scalability and complexity. It is possible to implement fixes but as the data keeps growing, new problems will arise. On the other hand, Big Data designed databases (noSQL) are actually a \"trade-off\" in comparison to standard models: they don't face problems when working with extended quantities of data, but they have additional complexity and lack tolerance for human mistakes.\n",
    "\n",
    "#### 2.\tWhat is “latency” when talking about databases? And why is it lower on Big Data designed databases? \n",
    "Latency on this context refers to the time between when a packet of data is sent and received. This \"delay\" is usually measured in milliseonds (ms) and it should always be as low as possible. It is lower on big data designed databases thanks to the lambda architecture: big data tools on their own usually don't have a lower latency than standard databases tools, but when combined properly using the principle of lambda architecture, the final result will have low latency.  \n",
    "\n",
    "#### 3.\tHow can “Lambda Architecture” be achieved in a way that it represents an edge in comparison to standard RDBMs?\n",
    "Lambda Architecture is achieved by using the correct combination of \"big data tools\" depending on the objective of our job. The edge achieved will be on the main three axes for databases: accuracy, latency and through-put.  \n",
    "\n",
    "#### 4. What is a “batch processing system”? Is it effective for incremental database systems?\n",
    "As the name says, it is the process in an algorithm processess \"batches\" of jobs. Yes, as it accomplishes the properties of big data database systems by being simple to use, robust and highly scalable. \n",
    "\n",
    "#### 5.\tWhat are the limitations of standard SQL databases when dealing with “Big Data”?\n",
    "SQL databases have many problems when working with big data depending on how big the data is. If the data is incremental, the problems will arise eventually on different milestones, such as: corrupt data, lack of fault-tolerance and finally, bottleneck problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tIs it always better to have the rawest data possible? Why?\n",
    "\n",
    "#### 2.\tWhat is “immutable data” and why is it better for Big Data designed systems?\n",
    "\n",
    "#### 3.\tWhy immutable data is “eternally true”, even when deleted? \n",
    "\n",
    "#### 4.\tWhy “semantic normalization” is not the same as “data normalization”?\n",
    "\n",
    "#### 5.\tHow does a “graph schema” compare to semi-structured text formats?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
